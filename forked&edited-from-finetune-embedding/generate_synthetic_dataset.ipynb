{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bdd511-4fc0-4bbd-9f4b-df76bbcb756a",
   "metadata": {},
   "source": [
    "# Generate Synthetic Dataset From MIMIC-IV-Note using Open-source LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fba52-b566-42ed-91f9-3a20be04f4e2",
   "metadata": {},
   "source": [
    "First, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08491e5-6273-4b09-b19f-bbf3276d562b",
   "metadata": {},
   "source": [
    "### Generate synthetic queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7f9f3-0aa2-4d04-bfb0-92d4eabc404b",
   "metadata": {},
   "source": [
    "Now, we use an LLM (Mistral-7B-Instruct-v0.1) to generate questions using each clinical note in the dataset as context.\n",
    "\n",
    "Each pair of (generated question, clinical note chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af162c19-4362-447c-9c57-e35d13cdb4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/75y/.conda/envs/mrc_meth/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bea63f-edc9-42fe-83fc-32bdfee75e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/75y/data_ragMimic/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48e338f-0cf0-497b-988b-5a7d112b3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CORPUS_FPATH = DATA_PATH+'train_corpus.json'\n",
    "VAL_CORPUS_FPATH = DATA_PATH+'val_corpus.json'\n",
    "\n",
    "TRAIN_QUERIES_FPATH = DATA_PATH+'train_queries.json'\n",
    "TRAIN_RELEVANT_DOCS_FPATH = DATA_PATH+'train_relevant_docs.json'\n",
    "\n",
    "VAL_QUERIES_FPATH = DATA_PATH+'val_queries.json'\n",
    "VAL_RELEVANT_DOCS_FPATH = DATA_PATH+'val_relevant_docs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfeb3cd-59fd-496c-a4f0-c269835123ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_CORPUS_FPATH, 'r+') as f:\n",
    "    train_corpus = json.load(f)\n",
    "\n",
    "with open(VAL_CORPUS_FPATH, 'r+') as f:\n",
    "    val_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05407fc1-17bf-4f6d-8006-4ca02bfb8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6b0bca079d44019f5f7bac484ea6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6725891a174506b8cc2ca6cb7f1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "pipeline_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    device=\"cuda:0\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c208f5-89dc-450a-9d62-75cc0800a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    pipeline,\n",
    "    corpus,\n",
    "    num_questions_per_chunk=2,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    # llm = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1', use_cache=True)\n",
    "    # OpenAI(model=)\n",
    "\n",
    "#     prompt_template = prompt_template or \"\"\"\\\n",
    "# Context information is below.\n",
    "\n",
    "# ---------------------\n",
    "# {context_str}\n",
    "# ---------------------\n",
    "\n",
    "# Given the context information and not prior knowledge.\n",
    "# generate only questions based on the below query.\n",
    "\n",
    "# You are a Medical Professional. Your task is to come up with \\\n",
    "# {num_questions_per_chunk} questions for finding clinical information about a patient. \\\n",
    "# The questions should be strictly about patient's medical history and diverse across the document. \\\n",
    "# Restrict the questions to the context information provided. \\\n",
    "# In your response, just list {num_questions_per_chunk} questions separated by new lines.\n",
    "# \"\"\"\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge. \\\n",
    "Generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided. \\\n",
    "In your response, just list {num_questions_per_chunk} questions separated by new lines.\n",
    "\"\"\"\n",
    "        \n",
    "    count = 0\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for node_id, text in tqdm(corpus.items()):\n",
    "        prompt = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n",
    "\n",
    "        response = pipeline(prompt, max_new_tokens=100, pad_token_id=2)[0][\"generated_text\"][len(prompt) :]\n",
    "        # print(response)\n",
    "        \n",
    "        result = str(response).strip().split(\"\\n\")\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0]\n",
    "        questions = [question for question in questions if question != '---------------------']\n",
    "        \n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [node_id]\n",
    "        count+=1\n",
    "        if count > 2: break\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84780125-1c09-4904-bce1-23586d012c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d3c0cb85694e01883d1853a96983e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_queries, train_relevant_docs = generate_queries(pipeline_gen, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb623bdf-f080-45ab-8e34-e0624332ca42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2b099c90-e685-465e-aa61-c33c4d48aaa9': \"What is the patient's name?\",\n",
       " 'c3fd91f4-ad9d-4977-8be2-5a3c608a471c': \"What is the patient's major surgical or invasive procedure?\",\n",
       " '9a80d116-30c5-4f03-8bd5-cd786a8d72ea': \"What was the patient's discharge diagnosis?\",\n",
       " 'd7ee8c4b-f8dc-4a29-89eb-16b4f47b79bb': \"What was the patient's discharge medication regimen?\",\n",
       " 'ba8232e1-f3dd-46c0-843a-f3636cc07d93': \"What is the patient's name?\",\n",
       " '724792d6-f33a-43d9-b465-3742bdb8e5f6': \"What is the patient's major surgical or invasive procedure?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9452dcfc-7084-496f-87eb-8c7be6a6dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5df1d701f34af49a4a09e8d03227c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_queries, val_relevant_docs = generate_queries(pipeline_gen, val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96087eb2-607b-4115-ab37-426bfcf6af1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(TRAIN_QUERIES_FPATH, 'w+') as f:\n",
    "    json.dump(train_queries, f)\n",
    "\n",
    "with open(TRAIN_RELEVANT_DOCS_FPATH, 'w+') as f:\n",
    "    json.dump(train_relevant_docs, f)\n",
    "\n",
    "with open(VAL_QUERIES_FPATH, 'w+') as f:\n",
    "    json.dump(val_queries, f)\n",
    "\n",
    "with open(VAL_RELEVANT_DOCS_FPATH, 'w+') as f:\n",
    "    json.dump(val_relevant_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71453dc5-25e0-45bf-9d86-5e72b3a891d5",
   "metadata": {},
   "source": [
    "### Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f465498-daa5-41b3-9ea3-8114254832b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_FPATH = DATA_PATH+'train_dataset.json'\n",
    "VAL_DATASET_FPATH = DATA_PATH+'val_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430e34b0-699d-4eec-a26d-6d100d81cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    'queries': train_queries,\n",
    "    'corpus': train_corpus,\n",
    "    'relevant_docs': train_relevant_docs,\n",
    "}\n",
    "\n",
    "val_dataset = {\n",
    "    'queries': val_queries,\n",
    "    'corpus': val_corpus,\n",
    "    'relevant_docs': val_relevant_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09071a2-6c32-408a-b971-39b5d6e42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(train_dataset, f)\n",
    "\n",
    "with open(VAL_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3459f-1d12-4b81-a7ee-f9f04ab87c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
