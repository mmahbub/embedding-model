{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408938d5-5b28-4f66-a6f8-bd31b53a3e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/75y/.conda/envs/mrc_meth/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import json\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb64c4d7-1d66-4094-8670-95482ce93b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    punc_list = [\",\", \":\", \";\", \"'\", \"\\\"\"]\n",
    "\n",
    "    for punc in punc_list:\n",
    "        text = re.sub(rf\"{punc}+\", punc, text)\n",
    "    \n",
    "    for punc in punc_list:\n",
    "        text = re.sub(rf\"\\n[{punc}]\", punc, text)\n",
    "        text = re.sub(rf\"\\n [{punc}]\", punc, text)\n",
    "        text = re.sub(rf\"[{punc}]\\n\", f\"{punc} \", text)\n",
    "        text = re.sub(rf\"[{punc}] \\n\", f\"{punc} \", text)\n",
    "    \n",
    "    repl = re.findall(r\"\\n[a-z]\", text)\n",
    "    for r in repl:\n",
    "        text = re.sub(r, ' '+r[-1], text)   \n",
    "        \n",
    "    repl = re.findall(r\"\\n [a-z]\", text)\n",
    "    for r in repl:\n",
    "        text = re.sub(r, ' '+r[-1], text)   \n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fea2511-390b-4ff1-8507-ddceae47fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_PATH = \"/mnt/scratch/shared_data/MIMIC-IV-NOTE/note/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf706bc3-94ca-4d30-936b-230d3661826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MIMIC_PATH+\"discharge.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235257cb-24b9-4597-a512-3b525d317e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['subject_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8f6fc4-f019-408f-939a-3f4330857471",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_subject = random.sample(list(set(df['subject_id'])), 5000)\n",
    "test_subject  = random.sample(list(set(df['subject_id'])-set(train_subject)), 100)\n",
    "assert set(train_subject).intersection(set(test_subject)) == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27864e73-6c67-4280-9d35-dbe3b403e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11440 207\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df['subject_id'].isin(train_subject)].reset_index(drop=True)\n",
    "df_test  = df[df['subject_id'].isin(test_subject)].reset_index(drop=True)\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2c9dd6-55e6-428b-885b-b8f285565150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:12<00:00, 923.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 207/207 [00:00<00:00, 905.88it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['clean_text'] = df_train['text'].progress_apply(clean_text)\n",
    "df_test['clean_text']  = df_test['text'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "83644817-42b5-4947-a7c7-611dc867a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Chunking . . . .\n",
      "Number of chunks: 3\n",
      "Chunking Completed . . . .\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "def note_to_chunk(tokenizer, text, max_length, doc_stride):\n",
    "    '''\n",
    "    Function to extract information using zero-shot prompting\n",
    "    '''\n",
    "    # tokenize prompt+note\n",
    "    input_ids = tokenizer(text, truncation=\"only_first\", padding='max_length',\n",
    "                          return_overflowing_tokens=True, stride=doc_stride,\n",
    "                          return_offsets_mapping=True, max_length=max_length)\n",
    "        \n",
    "    chunk_to_note_map = input_ids.pop(\"overflow_to_sample_mapping\")\n",
    "    input_ids = input_ids.input_ids\n",
    "    \n",
    "    chunked_text = tokenizer.batch_decode(input_ids)\n",
    "    chunked_text = [s.replace(\"[CLS]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"[SEP]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"[PAD]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"_ _ _\", \"___\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" \\ \", \"\\\\\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" / \", \"/\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" & \", \"&\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" : \", \":\") for s in chunked_text]\n",
    "    chunk_list = []\n",
    "    for s in chunked_text:\n",
    "        for r in re.findall(r\"\\n [a-z]\", s):\n",
    "            s = re.sub(r, '\\n'+r[-1], s)\n",
    "        chunk_list.append(s)\n",
    "    \n",
    "    chunk_list = [{'text':chunk} for chunk in chunk_list]\n",
    "    print(f\"Number of chunks: {len(chunk_list)}\")\n",
    "    return chunk_list\n",
    "\n",
    "print(f\"Started Chunking . . . .\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [AddedToken(\"\\n\")]})\n",
    "\n",
    "max_length = 512\n",
    "doc_stride = 128\n",
    "text = df_train['clean_text'].iloc[0]\n",
    "x = note_to_chunk(tokenizer, text, max_length, doc_stride)\n",
    "print(f\"Chunking Completed . . . .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d5ad90e-d50c-454d-a729-b3fbc31d2c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' \\nname:___ unit no:___ \\n \\nadmission date:___ discharge date:___ \\n \\ndate of birth:___ sex:f \\n \\nservice:surgery \\n \\nallergies:___ \\n \\nattending:___. \\n \\nchief complaint:nausea/vomiting \\n \\nmajor surgical or invasive procedure:band adjustment \\n \\n \\nhistory of present illness:ms. ___ is a ___ s/p lap band in ___ who prsents with a 1 week history of nausea, non - bilious non - bloody emesis of undigested food after eating, intolerance to solids/softs, hypersalivation, and moderate post - prandial epigastric discomfort. she denies fever, chills, hematemesis, brbpr, melena, diarrhea, or sympotoms of dehydration, but was recently evaluated for dizziness in an ed with a diagnosis given of bppv. of note, the patient underwent an unfill of her band from 5. 8 to 3. 8ml on \\n ___ for similar symptoms, the band was subseqently been filled to 4. 8 on ___, 5. 2 on ___, and most recently to 5. 6ml on \\n ___. \\n \\n \\npast medical history:pmhx:hyperlipidemia and with elevated triglyceride, iron deficiency anemia, irritable bowel syndrome, allergic rhinitis, dysmenorrhea, vitamin d deficiency, question of hypothyroidism with elevated tsh level, thalassemia trait, fatty liver and cholelithiasis by ultrasound study. a history of kissing tonsils that was associated with obstructive sleep apnea and gastroesophageal reflux, these have resolved completely after the tonsillectomy in ___. history of polycystic ovary syndrome \\n \\nsocial history:___ \\nfamily history:bladder ca ; with diabetes, breast neoplasia, colon ca, ovarian \\nca and sarcoma \\n \\nphysical exam:vs:temp:97. 9, hr:72, bp:113/64, rr:16, o2sat:100 % ra \\ngen:a&o, nad \\nheent:no scleral icterus, mmm \\ncv '},\n",
       " {'text': ' apnea and gastroesophageal reflux, these have resolved completely after the tonsillectomy in ___. history of polycystic ovary syndrome \\n \\nsocial history:___ \\nfamily history:bladder ca ; with diabetes, breast neoplasia, colon ca, ovarian \\nca and sarcoma \\n \\nphysical exam:vs:temp:97. 9, hr:72, bp:113/64, rr:16, o2sat:100 % ra \\ngen:a&o, nad \\nheent:no scleral icterus, mmm \\ncv:rrr \\npulm:no w/r/c, no increased work of breathing \\nabd:soft, nondistended, non - tender to palpation in epigastric region, no rebound or guarding, palpable port \\next:no ___ edema, warm and well perfused \\n \\n \\npertinent results:___ 12:16am plt count - 243 \\n ___ 12:16am neuts - 46. 0 ___ monos - 6. 9 eos - 1. 8 \\nbasos - 0. 5 im ___ absneut - 4. 88 abslymp - 4. 72 * absmono - 0. 73 \\nabseos - 0. 19 absbaso - 0. 05 \\n ___ 12:16am estgfr - using this \\n ___ 01:02am urine mucous - rare \\n ___ 01:02am urine hyaline - 1 * \\n ___ 01:02am urine rbc - 4 * wbc - 4 bacteria - mod yeast - none \\nepi - 11 \\n ___ 01:02am urine blood - neg nitrite - neg protein - 30 \\nglucose - neg ketone - neg bilirubin - neg urobilngn - neg ph - 6. 5 \\nleuk - tr \\n ___ 01:02am urine color - yellow appear - hazy sp ___ \\n ___ 01:02am urine ucg - negative \\n ___ 01:02am urine hours - random \\n ___ 01:02am urine hours - random \\n \\nbrief hospital course:___ was admitted from ed on ___ for nausea and vomiting after any po intake. of note, she has had '},\n",
       " {'text': ' neg protein - 30 \\nglucose - neg ketone - neg bilirubin - neg urobilngn - neg ph - 6. 5 \\nleuk - tr \\n ___ 01:02am urine color - yellow appear - hazy sp ___ \\n ___ 01:02am urine ucg - negative \\n ___ 01:02am urine hours - random \\n ___ 01:02am urine hours - random \\n \\nbrief hospital course:___ was admitted from ed on ___ for nausea and vomiting after any po intake. of note, she has had similar symptomes last year. she was started on iv fluids for rehydration. her laboratory values were unremarkable on admission and her symptoms gradually improved with anti - emetic medications and iv fluid therapy. she was back to her baseline clinical status after unfilling the band by 1. 5cc. water challenge test was done after band adjustment and was negative for any pain, nausea or vomiting. she was discharged in good condition with instructions to follow up with dr. ___ \\n ___ after 2. \\n \\n \\ndischarge medications:1. lorazepam 0. 5 mg po bid:prn anxiety \\n 2. buspirone 5 mg po tid \\n \\n \\ndischarge disposition:home \\n \\ndischarge diagnosis:nausea and vomiting due to tight band \\n \\n \\ndischarge condition:mental status:clear and coherent. \\nlevel of consciousness:alert and interactive. \\nactivity status:ambulatory - independent. \\n \\n \\ndischarge instructions:you were admitted to ___ for your nausea and vomiting. your band was tight enough to cause your nausea and vomiting, 1. 5 cc has been taken out from your band in which 2. 5cc total left. you subsequently tolerated a water bolus test. you have been deemed fit to be discharged from the hospital. please return if your nausea becomes untolerable or you start vomiting again. please continue taking your home medications. \\nthank you for letting us participate in your healthcare. \\n \\n \\nfollowup instructions:___ \\n                                                                          '}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87317251-3848-4f7e-997d-0e44860c4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "noteid_to_finalnote_train = {}\n",
    "for i,row in df_train.iterrows():\n",
    "    noteid_to_finalnote_train[row['note_id']] = f\"\"\"The following note with note_id {row['note_id']} was charted on {row['charttime']} for patient {row['subject_id']}.\\n\n",
    "Note with note_id {row['note_id']}: {row['clean_text']}\"\"\"\n",
    "\n",
    "noteid_to_finalnote_test = {}\n",
    "for i,row in df_test.iterrows():\n",
    "    noteid_to_finalnote_test[row['note_id']] = f\"\"\"The following note with note_id {row['note_id']} was charted on {row['charttime']} for patient {row['subject_id']}.\\n\n",
    "Note with note_id {row['note_id']}: {row['clean_text']}\"\"\"\n",
    "    \n",
    "df_train['final_note'] = df_train['note_id'].map(noteid_to_finalnote_train)\n",
    "df_test['final_note']  = df_test['note_id'].map(noteid_to_finalnote_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598dffed-98a2-44a3-aa11-406155545f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/75y/data_ragMimic/data/\"\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    with open(DATA_PATH+f\"train_{i}.txt\", \"w\") as f:\n",
    "        f.write(df_train['final_note'].iloc[i])\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    with open(DATA_PATH+f\"test_{i}.txt\", \"w\") as f:\n",
    "        f.write(df_test['final_note'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a958008b-1d39-41ec-b125-cd9bfa9207e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(files, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Loading files {files}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    if verbose:\n",
    "        print(f'Loaded {len(docs)} docs')\n",
    "    \n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Parsed {len(nodes)} nodes')\n",
    "\n",
    "    corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e334bec-c2cd-4fbf-86fe-5571d76bd87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILES = [DATA_PATH+f\"train_{i}.txt\" for i in range(len(df_train))]\n",
    "VAL_FILES   = [DATA_PATH+f\"test_{i}.txt\" for i in range(len(df_test))]\n",
    "\n",
    "train_corpus = load_corpus(TRAIN_FILES, verbose=False)\n",
    "val_corpus = load_corpus(VAL_FILES, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4271383e-d227-4434-a0cc-e9b75d1f9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/75y/data_ragMimic/data/\"\n",
    "\n",
    "TRAIN_CORPUS_FPATH = DATA_PATH+'train_corpus.json'\n",
    "VAL_CORPUS_FPATH = DATA_PATH+'val_corpus.json'\n",
    "\n",
    "with open(TRAIN_CORPUS_FPATH, 'w+') as f:\n",
    "    json.dump(train_corpus, f)\n",
    "\n",
    "with open(VAL_CORPUS_FPATH, 'w+') as f:\n",
    "    json.dump(val_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbbdcd-1931-48cb-b727-abb315542773",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### trial code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5a245-5eb5-4cba-97fd-129a9b863acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # !mkdir -p 'data/10k/'\n",
    "# # # !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "# # # !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n",
    "\n",
    "# # import json\n",
    "\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# # from llama_index.core.node_parser import SimpleNodeParser\n",
    "# from llama_index.core.node_parser import TokenTextSplitter\n",
    "# from llama_index.core.schema import MetadataMode\n",
    "# # TRAIN_FILES = ['data/10k/lyft_2021.pdf']\n",
    "# # VAL_FILES = ['data/10k/uber_2021.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f214da-e444-40a9-8275-83a88b0474bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_corpus(files, verbose=False):\n",
    "#     if verbose:\n",
    "#         print(f\"Loading files {files}\")\n",
    "\n",
    "#     reader = SimpleDirectoryReader(input_files=files)\n",
    "#     docs = reader.load_data()\n",
    "#     if verbose:\n",
    "#         print(f'Loaded {len(docs)} docs')\n",
    "    \n",
    "#     parser = SimpleNodeParser.from_defaults()\n",
    "#     nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "#     nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f'Parsed {len(nodes)} nodes')\n",
    "\n",
    "#     corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3e238-7914-45fb-a522-08d5747f3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_corpus(docs, verbose=False):\n",
    "#     if verbose:\n",
    "#         print(f'Loaded {len(docs)} docs')\n",
    "    \n",
    "#     # parser = SimpleNodeParser.from_defaults()\n",
    "#     # nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "#     splitter = TokenTextSplitter(\n",
    "#         chunk_size=1024,\n",
    "#         chunk_overlap=256,\n",
    "#         separator=\" \",\n",
    "#     )\n",
    "#     nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f'Parsed {len(nodes)} nodes')\n",
    "\n",
    "#     corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd84960-232e-4513-ac31-de5b1865a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = load_corpus(TRAIN_FILES, verbose=True)\n",
    "# val_corpus = load_corpus(VAL_FILES, verbose=True)\n",
    "\n",
    "# # train_corpus = load_corpus(df_train, verbose=True)\n",
    "# # val_corpus = load_corpus(df_test, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f1d8c-eb6c-41ab-9fd0-9e16e6ffb17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
