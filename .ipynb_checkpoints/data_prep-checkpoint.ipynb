{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faff46d2-fd49-4008-ad12-f091caf12be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/75y/.conda/envs/mrc_meth/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# pd.set_option('max_colwidth', 1000)\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tokenizers import AddedToken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e76c426-b060-4fff-a0ae-14c911f8f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    punc_list = [\",\", \":\", \";\", \"'\", \"\\\"\"]\n",
    "\n",
    "    for punc in punc_list:\n",
    "        text = re.sub(rf\"{punc}+\", punc, text)\n",
    "    \n",
    "    for punc in punc_list:\n",
    "        text = re.sub(rf\"\\n[{punc}]\", punc, text)\n",
    "        text = re.sub(rf\"\\n [{punc}]\", punc, text)\n",
    "        text = re.sub(rf\"[{punc}]\\n\", f\"{punc} \", text)\n",
    "        text = re.sub(rf\"[{punc}] \\n\", f\"{punc} \", text)\n",
    "    \n",
    "    repl = re.findall(r\"\\n[a-z]\", text)\n",
    "    for r in repl:\n",
    "        text = re.sub(r, ' '+r[-1], text)   \n",
    "        \n",
    "    repl = re.findall(r\"\\n [a-z]\", text)\n",
    "    for r in repl:\n",
    "        text = re.sub(r, ' '+r[-1], text)   \n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e71a9e-cc88-41a0-8a7d-e4a23eaa45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_to_chunk(tokenizer, text, max_length, doc_stride, save_to_json, OUTPUT_PATH, FILENAME):\n",
    "    '''\n",
    "    Function to create chunks\n",
    "    '''\n",
    "    input_ids = tokenizer(text, truncation=\"only_first\", padding='max_length',\n",
    "                          return_overflowing_tokens=True, stride=doc_stride,\n",
    "                          return_offsets_mapping=True, max_length=max_length)\n",
    "        \n",
    "    chunk_to_note_map = input_ids.pop(\"overflow_to_sample_mapping\")\n",
    "    input_ids = input_ids.input_ids\n",
    "    \n",
    "    chunked_text = tokenizer.batch_decode(input_ids)\n",
    "    chunked_text = [s.replace(\"[CLS]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"[SEP]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"[PAD]\", \"\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\"_ _ _\", \"___\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" \\ \", \"\\\\\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" / \", \"/\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" & \", \"&\") for s in chunked_text]\n",
    "    chunked_text = [s.replace(\" : \", \":\") for s in chunked_text]\n",
    "    chunk_list = []\n",
    "    for s in chunked_text:\n",
    "        for r in re.findall(r\"\\n [a-z]\", s):\n",
    "            s = re.sub(r, '\\n'+r[-1], s)\n",
    "        chunk_list.append(s)\n",
    "    \n",
    "    chunk_list = [{'text':chunk} for chunk in chunk_list]\n",
    "    print(f\"Number of chunks: {len(chunk_list)}\")\n",
    "\n",
    "    if save_to_json:\n",
    "        with open(f'{OUTPUT_PATH}+{FILENAME}', 'w') as outfile:\n",
    "            for entry in chunk_list:\n",
    "                json.dump(entry, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963e2d11-05c0-4c27-8564-62bd57a38425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1255.70it/s]\n"
     ]
    }
   ],
   "source": [
    "MIMIC_PATH = \"/mnt/scratch/shared_data/MIMIC-IV-NOTE/note/\"\n",
    "df = pd.read_csv(MIMIC_PATH+\"discharge.csv.gz\").head(10)\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43fc3e85-7503-4eab-81b9-089e8593cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Chunking . . . .\n",
      "Number of chunks: 64\n",
      "Chunking Completed . . . .\n"
     ]
    }
   ],
   "source": [
    "print(f\"Started Chunking . . . .\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [AddedToken(\"\\n\")]})\n",
    "\n",
    "max_length = 512\n",
    "doc_stride = 128\n",
    "text       = df['clean_text'].values.tolist()\n",
    "\n",
    "save_to_json = True \n",
    "OUTPUT_PATH  = \"../\"\n",
    "FILENAME     = \"pretrain_data.jsonl\"\n",
    "\n",
    "note_chunks = note_to_chunk(tokenizer, text, max_length, doc_stride, save_to_json, OUTPUT_PATH, FILENAME)\n",
    "print(f\"Chunking Completed . . . .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b3d96-1cd5-4ad9-bcba-85b5abe4316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
